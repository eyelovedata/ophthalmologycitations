}
cv_mse=mean(mse_tmp)
param_mse[i-param_start+1,2]=cv_mse
param_mse[i-param_start+1,1]=i
}
plot(param_mse[,1],param_mse[,2], ylab="error", xlab="parameter")
param_mse[which.min(param_mse[,2]),]
fit=randomForest(P~. -logP, data=trainfactor, ntree=1000, mtry=6)
importance(fit)
library(gbm)
gbmfit=gbm(P~. -logP, data=trainfactor, n.trees=400, interaction.depth=3, distribution="gaussian", cv.folds=5, n.minobsinnode=3)
best.iter<-gbm.perf(gbmfit,method="cv")
print(best.iter)
gbmfit=gbm(P~. -logP, data=trainfactor, n.trees=1000, interaction.depth=3, distribution="gaussian", cv.folds=5, n.minobsinnode=3)
best.iter<-gbm.perf(gbmfit,method="cv")
print(best.iter)
summary(gbmfit)
View(gbmfit)
View(gbmfit)
gbmfit=gbm(P~. -logP, data=trainfactor, n.trees=1000, interaction.depth=6, distribution="gaussian", cv.folds=5, n.minobsinnode=3, shrinkage=.04)
gbmfit=gbm(P~. -logP, data=trainfactor, n.trees=1000, interaction.depth=6, distribution="gaussian", cv.folds=5, n.minobsinnode=3, shrinkage=0.04)
best.iter<-gbm.perf(gbmfit,method="cv")
print(best.iter)
summary(gbmfit)
library(caret)
?trainControl
nl = nrow(trainfactor)
max(0.01, 0.1*min(1, nl/10000))
# Max Value for interaction.depth
floor(sqrt(ncol(trainfactor)))
gbmGrid <-  expand.grid(interaction.depth = c(1, 3, 6, 9, 10),
n.trees = (0:50)*50,
shrinkage = seq(.0005, .05,.0005),
n.minobsinnode = c(3,5,7,10,15)) # you can also put something        like c(5, 10, 15, 20)
fitControl <- trainControl(method = "repeatedcv",
repeats = 5,
selectionFunction='best',
savePredictions=TRUE)
gbmModel=train(P~. -logP, data=trainfactor, method="gbm", trControl=fitControl, tuneGrid=gbmGrid)
nl = nrow(trainfactor)
max(0.01, 0.1*min(1, nl/10000))
# Max Value for interaction.depth
floor(sqrt(ncol(trainfactor)))
gbmGrid <-  expand.grid(interaction.depth = c(1, 3, 6, 9, 10),
n.trees = (0:50)*50,
shrinkage = seq(.0005, .05,.0005),
n.minobsinnode = c(3,5,7,10,15)) # you can also put something        like c(5, 10, 15, 20)
fitControl <- trainControl(method = "repeatedcv",
repeats = 5,
selectionFunction='best',
savePredictions=TRUE)
gbmModel=train(P~. -logP, data=trainfactor, method="gbm", distribution="gaussian", trControl=fitControl, tuneGrid=gbmGrid, verbose=TRUE, bag.fraction=0.8)
nl = nrow(trainfactor)
max(0.01, 0.1*min(1, nl/10000))
# Max Value for interaction.depth
floor(sqrt(ncol(trainfactor)))
gbmGrid <-  expand.grid(interaction.depth = c(1, 3, 6, 9, 10),
n.trees = (0:50)*50,
shrinkage = seq(.0005, .05,.0005),
n.minobsinnode = c(3,5,7,10,15)) # you can also put something        like c(5, 10, 15, 20)
fitControl <- trainControl(method = "repeatedcv",
repeats = 5,
selectionFunction='best',
savePredictions=TRUE)
gbmModel=train(P~. -logP, data=trainfactor, method="gbm", distribution="gaussian", trControl=fitControl, tuneGrid=gbmGrid, verbose=TRUE, trainfraction=0.8)
nl = nrow(trainfactor)
max(0.01, 0.1*min(1, nl/10000))
# Max Value for interaction.depth
floor(sqrt(ncol(trainfactor)))
gbmGrid <-  expand.grid(interaction.depth = c(1, 3, 6, 9, 10),
n.trees = (0:50)*50,
shrinkage = seq(.0005, .05,.0005),
n.minobsinnode = c(3,5,7,10,15)) # you can also put something        like c(5, 10, 15, 20)
fitControl <- trainControl(method = "repeatedcv",
repeats = 5,
selectionFunction='best',
savePredictions=TRUE)
gbmModel=train(P~. -logP, data=trainfactor, method="gbm", distribution="gaussian", trControl=fitControl, tuneGrid=gbmGrid, verbose=TRUE, metric="RMSE", bag.fraction=0.8)
nl = nrow(trainfactor)
max(0.01, 0.1*min(1, nl/10000))
# Max Value for interaction.depth
floor(sqrt(ncol(trainfactor)))
gbmGrid <-  expand.grid(interaction.depth = c(1, 3, 6, 9, 10),
n.trees = (0:50)*50,
shrinkage = seq(.0005, .05,.0005),
n.minobsinnode = c(3,5,7,10,15)) # you can also put something        like c(5, 10, 15, 20)
fitControl <- trainControl(method = "repeatedcv",
repeats = 5,
selectionFunction='best',
savePredictions=TRUE)
gbmModel=train(P~. -logP, data=trainfactor, method="gbm", distribution="gaussian", trControl=fitControl, tuneGrid=gbmGrid, verbose=TRUE, metric="RMSE", train.fraction=0.8)
nl = nrow(trainfactor)
max(0.01, 0.1*min(1, nl/10000))
# Max Value for interaction.depth
floor(sqrt(ncol(trainfactor)))
gbmGrid <-  expand.grid(interaction.depth = c(1, 3, 6, 9, 10),
n.trees = (0:50)*50,
shrinkage = seq(.0005, .05,.0005),
n.minobsinnode = c(3,5,7,10,15)) # you can also put something        like c(5, 10, 15, 20)
fitControl <- trainControl(method = "repeatedcv",
repeats = 5,
selectionFunction='best',
savePredictions=TRUE)
gbmModel=train(P~. -logP, data=trainfactor, method="gbm", distribution="gaussian", trControl=fitControl, tuneGrid=gbmGrid, verbose=FALSE, metric="RMSE", nTrain = round(nrow(trainfactor) *.8))
nl = nrow(trainfactor)
max(0.01, 0.1*min(1, nl/10000))
# Max Value for interaction.depth
floor(sqrt(ncol(trainfactor)))
gbmGrid <-  expand.grid(interaction.depth = c(1, 3, 5, 7),
n.trees = (0:50)*30,
shrinkage = seq(.0005, .05,.0005),
n.minobsinnode = c(3,5,7,10,15)) # you can also put something        like c(5, 10, 15, 20)
fitControl <- trainControl(method = "repeatedcv",
repeats = 5,
selectionFunction='best',
savePredictions=TRUE)
gbmModel=train(P~. -logP, data=trainfactor, method="gbm", distribution="gaussian", trControl=fitControl, tuneGrid=gbmGrid, verbose=FALSE, metric="RMSE", nTrain = round(nrow(trainfactor) *.8))
plot(gbmModel)
gbmModel$modelInfo
gbmModel$finalModel
print(gbmModel)
View(gbmModel)
View(gbmModel)
gbmModel$results[870,]
which.min(gbmModel$Results$RMSE)
gbmModel$Results$RMSE
gbmModel$results
gbmModelCV=gbmModel$results
which.min(gbmModelCV$RMSE)
gbmModelCV[which.min(gbmMOdelCV$RMSE)]
gbmModelCV[which.min(gbmModelCV$RMSE)]
gbmModelCV[which.min(gbmModelCV$RMSE),]
(54101.97)^2
predict(gbmModel$finalModel, newdata=testfactor)
testfactor$LotRatio=test$S/test$L
testfactor$RoomRatio=test$S/test$R
testfactor$logP=log(testfactor$P)
predict(gbmModel$finalModel, newdata=testfactor)
gbmbest=gbm(P~. -logP, data=trainfactor, distribution="gaussian", n.trees=870, interaction.depth=3, shrinkage=0.023, n.minobsinnode=7)
testpred=predict(gbmbest, newdata=testfactor, n.trees=870, interaction.depth=3, shrinkage=0.023, n.minobsinnode=7)
testpred
output=data.frame(Id=testfactor[,1], Predicted=testpred)
testpredall$gbmcv=testpred
write.table(output, "gbmcvoutput.csv", row.names=FALSE, sep=",")
library(randomForest)
set.seed(1)
nfolds=5
folds_i=sample(rep(1:nfolds, length.out = 275))
mse_tmp <- c()
param_start = 2
param_end = 8
param_mse = matrix(NA, param_end-param_start+1,2)
#i is the tuning parameter, looped over integer values from start to end.
for (i in param_start:param_end){
for (k in 1:nfolds) {
test_i <- which(folds_i == k)
train_xy <- trainfactor[-test_i, ]
test_xy <- trainfactor[test_i, ]
x <- train_xy[,-1]
y <- train_xy$P
fit=randomForest(P~. -logP, data=train_xy, ntree=1000, mtry=i)
testpred=predict(fit, test_xy, ntree=1000, mtry=i)
mse=mean((testpred-test_xy$P)^2)
mse_tmp[k] <- mse
}
cv_mse=mean(mse_tmp)
param_mse[i-param_start+1,2]=cv_mse
param_mse[i-param_start+1,1]=i
}
plot(param_mse[,1],param_mse[,2], ylab="error", xlab="parameter")
param_mse[which.min(param_mse[,2]),]
fit=randomForest(P~. -logP, data=trainfactor, ntree=1000, mtry=6)
importance(fit)
fit=randomForest(P~. , data=finaltrainfactor[,1:9], mtry=6)
fit=randomForest(P~. , data=trainfactor, mtry=6)
testpred=predict(fit, testfactor, mtry=6)
fit=randomForest(P~. , data=trainfactor, mtry=6)
testpred=predict(fit, testfactor, mtry=6)
fit=randomForest(P~. , data=trainfactor, mtry=6)
testpred=predict(fit, newdata=testfactor, mtry=6)
testfactor
fit=randomForest(P~. , data=trainfactor, mtry=6)
testpred=predict(fit, newdata=testfactor[2:12], mtry=6)
fit=randomForest(P~. , data=trainfactor, mtry=6)
testpred=predict(fit, newdata=testfactor[,2:12], mtry=6)
fit=randomForest(P~ -logP , data=trainfactor, mtry=6)
fit=randomForest(P~ . -logP , data=trainfactor, mtry=6, ntree=1000)
testpred=predict(fit, newdata=testfactor[,2:12], mtry=6)
output=data.frame(Id=testfactor[,1], Predicted=testpred)
testpredall$rf=testpred
#write.table(output, "randomForestoutput.csv", row.names=FALSE, sep=",")
fit=knnreg(P~S+R, data=trainfactor, k=8)
testpred=predict(fit, testfactor)
output=data.frame(Id=testfactor[,1], Predicted=testpred)
testpredall$knn=testpred
#write.table(output, "knnregoutput.csv", row.names=FALSE, sep=",")
cols=c(2:4)
ensemblepred=rowMeans(testpredall[,cols])
output=data.frame(Id=testpredall$Id, Predicted=ensemblepred)
write.table(output, "ensemblerfgbmcvnewvars", row.names=FALSE, sep=",")
knitr::opts_chunk$set(echo = TRUE)
test<-read.csv("test.csv", header=TRUE)
train<-read.csv("train.csv", header=TRUE)
#we don't really need the ID variable for the training set
train <-train[,2:10]
trainfactor<-train
testfactor<-test
trainfactor$R<-as.factor(trainfactor$R)
trainfactor$Sch<-as.factor(trainfactor$Sch)
trainfactor$C<-as.factor(trainfactor$C)
testfactor$R<-as.factor(testfactor$R)
testfactor$Sch<-as.factor(testfactor$Sch)
testfactor$C<-as.factor(testfactor$C)
trainfactor$S<-scale(train$S)
trainfactor$Y<-scale(train$Y)
trainfactor$YB<-scale(train$YB)
trainfactor$L<-scale(train$L)
testfactor$S<-scale(test$S)
testfactor$Y<-scale(test$Y)
testfactor$YB<-scale(test$YB)
testfactor$L<-scale(test$L)
#a ratio between square footage and lot size
trainfactor$LotRatio=train$S/train$L
train$LotRatio=train$S/train$L
#a ratio between square footage and number of rooms - like average size of rooms ish
trainfactor$RoomRatio=train$S/train$R
train$RoomRatio=train$S/train$R
testfactor$LotRatio=test$S/test$L
testfactor$RoomRatio=test$S/test$R
testfactor$logP=log(testfactor$P)
trainfactor$logP=log(trainfactor$P)
hist(trainfactor$logP)
knitr::opts_chunk$set(echo = TRUE)
library(twang)
library(glmnet)
library(survival)
library(tidyr)
df=read.csv('C:/Users/Sophia/Documents/ResearchPHI/STRIDE_FULL/predictglaucsurgoutcomes/predictglaucsurgoutcomes-structured-95.csv')
df=dplyr::filter(df, fudayscox>0)
filter(df, split=0)
dplyr::filter(df, split=0)
dplyr::filter(df, split==0)
df_train<-dplyr::filter(df, split==0)
df_dev<-dplyr::filter(df, split==1)
X<-as.matrix(df_train[,6:167])
cv.fit <- cv.glmnet(X, Surv(df$fudayscox, df$failcox), family="cox", maxit = 1000)
df$gsurg=as.factor(df$gsurg)
df_train<-dplyr::filter(df, split==0)
df_dev<-dplyr::filter(df, split==1)
X<-as.matrix(df_train[,6:167])
cv.fit <- cv.glmnet(X, Surv(df_train$fudayscox, df_train$failcox), family="cox", maxit = 1000)
plot(cv.fit)
cv.fit$lambda.min
Coefficients <- coef(cv.fit, s = cv.fit$lambda.min)
Active.Index <- which(Coefficients != 0)
Active.Coefficients <- Coefficients[Active.Index]
cv.fit$lambda.min
colnames(X)[Active.Index]
Active.Coefficients
library(ggfortify)
#library(ggfortify)
library(survival)
fit <- survfit(Surv(fudayscox, failcox) ~ gsurg, data = df_train)
autoplot(fit)
install.packages("ggfortify")
library(ggfortify)
library(survival)
fit <- survfit(Surv(fudayscox, failcox) ~ gsurg, data = df_train)
autoplot(fit)
mnps.df <- mnps(gsurg ~ . -fail -failcox -fudayscox -pat_deid -proc_date -split,
+ data = df_train,
mnps.df <- mnps(gsurg ~ . -fail -failcox -fudayscox -pat_deid -proc_date -split, data = df_train,
+ estimand = "ATE",
mnps.df <- mnps(gsurg ~ . -fail -failcox -fudayscox -pat_deid -proc_date -split, data = df_train, estimand = "ATE", verbose = TRUE, stop.method = c("es.mean", "ks.mean"), n.trees = 3000)
mnps.df <- mnps(gsurg ~ . -fail -failcox -fudayscox -pat_deid -proc_date -split -gsurg, data = df_train, estimand = "ATE", verbose = TRUE, stop.method = c("es.mean", "ks.mean"), n.trees = 3000)
mnps.df <- mnps(gsurg ~ . medcountbase + oralcaibase + agestandard, data = df_train, estimand = "ATE", verbose = TRUE, stop.method = c("es.mean", "ks.mean"), n.trees = 3000)
mnps.df <- mnps(gsurg ~ medcountbase + oralcaibase + agestandard, data = df_train, estimand = "ATE", verbose = TRUE, stop.method = c("es.mean", "ks.mean"), n.trees = 3000)
plot(mnps.df, plots = 1)
plot(mnps.AOD, plots = 2, subset = "es.mean")
plot(mnps.df, plots = 2, subset = "es.mean")
mnps.df <- mnps(gsurg ~ medcountbase + oralcaibase + agestandard, data = df_train, estimand = "ATE", verbose = FALSE, stop.method = c("es.mean", "ks.mean"), n.trees = 5000)
plot(mnps.df, plots = 1)
plot(mnps.df, plots = 2, subset = "es.mean")
plot(mnps.df, plots = 2, subset = "ks.mean")
mnps.df <- mnps(df_train$gsurg ~ df_train[,6:167] -df_train$gsurg, data = df_train, estimand = "ATE", verbose = FALSE, stop.method = c("es.mean", "ks.mean"), n.trees = 5000)
mnps.df <- mnps(df_train$gsurg ~ . -df_train$gsurg, data = df_train[,6:167], estimand = "ATE", verbose = FALSE, stop.method = c("es.mean", "ks.mean"), n.trees = 5000)
mnps.df <- mnps(df_train$gsurg ~ . , data = df_train[,6:167], estimand = "ATE", verbose = FALSE, stop.method = c("es.mean", "ks.mean"), n.trees = 5000)
X_train<-df_train[,6:167]
mnps.df <- mnps(df_train$gsurg ~ . , data = X_train, estimand = "ATE", verbose = FALSE, stop.method = c("es.mean", "ks.mean"), n.trees = 5000)
colnames(X_train)
df=read.csv('C:/Users/Sophia/Documents/ResearchPHI/STRIDE_FULL/predictglaucsurgoutcomes/predictglaucsurgoutcomes-structured-95.csv')
df=dplyr::filter(df, fudayscox>0)
df$gsurg=as.factor(df$gsurg)
df_train<-dplyr::filter(df, split==0)
df_dev<-dplyr::filter(df, split==1)
X<-as.matrix(df_train[,6:157])
cv.fit <- cv.glmnet(X, Surv(df_train$fudayscox, df_train$failcox), family="cox", maxit = 1000)
plot(cv.fit)
cv.fit$lambda.min
Coefficients <- coef(cv.fit, s = cv.fit$lambda.min)
Active.Index <- which(Coefficients != 0)
Active.Coefficients <- Coefficients[Active.Index]
colnames(X)[Active.Index]
Active.Coefficients
library(ggfortify)
library(survival)
fit <- survfit(Surv(fudayscox, failcox) ~ gsurg, data = df_train)
autoplot(fit)
X_train<-df_train[,6:157]
colnames(X_train)
mnps.df <- mnps(df_train$gsurg ~ medcountbase + oralcaibase + agestandard + gender_Female + raceth_Asian + raceth_Black + raceth_Hispanic +raceth_Other + raceth_White + bcvalogmarworst + thi + cctlast + cdrworst + surgnumpereye + bcvalogmarworstmissing + thimissing + cctlastmissing + cdrworstmissing
, data = X_train, estimand = "ATE", verbose = FALSE, stop.method = c("es.mean", "ks.mean"), n.trees = 5000)
plot(mnps.df, plots = 1)
plot(mnps.df, plots = 2, subset = "es.mean")
df_train$w <- get.weights(mnps.df, stop.method = "es.mean")
design.mnps <- svydesign(ids=~1, weights=~w, data=df_train)
glm1 <- svyglm(failcox ~ as.factor(gsurg), design = design.mnps)
summary(glm1)
glm1 <- svyglm(Surv(fudayscox, failcox) ~ as.factor(gsurg), design = design.mnps)
glm1 <- svycoxph(Surv(fudayscox, failcox) ~ as.factor(gsurg), design = design.mnps)
summary(glm1)
autoplot(glm1)
as.formula(c("~", paste(colnames(X_train), collapse = "+")))
df=read.csv('C:/Users/Sophia/Documents/ResearchPHI/STRIDE_FULL/predictglaucsurgoutcomes/predictglaucsurgoutcomes-structured-95.csv')
df$gender_Male <- NULL
df=dplyr::filter(df, fudayscox>0)
df$gsurg=as.factor(df$gsurg)
df_train<-dplyr::filter(df, split==0)
df_dev<-dplyr::filter(df, split==1)
X<-as.matrix(df_train[,6:156])
X_train<-df_train[,6:156]
colnames(X_train)
colnames(select(X_train, -gsurg)))
colnames(select(X_train, -gsurg))
colnames(dplyr::select(X_train, -gsurg))
as.formula(c("~", colnames(dplyr::select(X_train, -gsurg)), collapse = "+")))
as.formula(c("~", colnames(dplyr::select(X_train, -gsurg)), collapse = "+"))
as.formula(c("~", paste(colnames(dplyr::select(X_train, -gsurg)), collapse = "+")))
as.formula(c("gsurg~", paste(colnames(dplyr::select(X_train, -gsurg)), collapse = "+")))
myformula=as.formula(c("gsurg~", paste(colnames(dplyr::select(X_train, -gsurg)), collapse = "+")))
myformula=as.formula(c("gsurg~", paste(colnames(dplyr::select(X_train, -gsurg)), collapse = "+")))
myformula
mnps.df <- mnps(myformula, data = X_train, estimand = "ATE", verbose = FALSE, stop.method = c("es.mean", "ks.mean"), n.trees = 3000)
plot(mnps.df, plots = 1)
plot(mnps.df, plots = 2, subset = "es.mean")
cox1 <- svycoxph(Surv(fudayscox, failcox) ~ as.factor(gsurg), design = design.mnps)
summary(cox1)
knitr::opts_chunk$set(echo = TRUE)
fit <- svykm(Surv(fudayscox, failcox) ~ as.factor(gsurg), design = design.mnps)
library(twang)
library(glmnet)
library(survival)
library(tidyr)
library(survey)
fit <- svykm(Surv(fudayscox, failcox) ~ as.factor(gsurg), design = design.mnps)
autoplot(fit)
library(twang)
library(glmnet)
library(survival)
library(tidyr)
library(survey)
library(ggfortify)
cox1 <- svycoxph(Surv(fudayscox, failcox) ~ as.factor(gsurg), design = design.mnps)
summary(cox1)
fit <- svykm(Surv(fudayscox, failcox) ~ as.factor(gsurg), design = design.mnps)
autoplot(fit)
fit <- svykm(Surv(fudayscox, failcox) ~ as.factor(gsurg), design = design.mnps)
svyjskm(fit)
install.packages('jskm')
fit <- svykm(Surv(fudayscox, failcox) ~ as.factor(gsurg), design = design.mnps)
summary(fit)
svyjskm(fit)
library(jskm)
fit <- svykm(Surv(fudayscox, failcox) ~ as.factor(gsurg), design = design.mnps)
summary(fit)
svyjskm(fit)
library(jskm)
fit <- svykm(Surv(fudayscox, failcox) ~ gsurg, design = design.mnps)
summary(fit)
svyjskm(fit)
summary(mnps.df)
setwd("C:/Users/Sophia/OneDrive/Documents/Research/PubmedOph/ophthalmologycitations")
knitr::opts_chunk$set(echo = TRUE)
df_scopus_csv <- read.csv('dfscopus-2.csv')
View(df_scopus_csv)
View(df_scopus_csv)
df_scopus_cv = subset(df_scopus_csv, select = -c(index,authorlist,authoridlist) )
df_scopus_cv = subset(df_scopus_csv, select = -c(index,authorlist,authoridlist))
View(df_scopus_cv)
include(tidyverse)
library(tidyverse)
df <- read.csv('dfscopus-2.csv')
select(df, -index -authorlist -authoridlist)
select(df, -index, -authorlist, -authoridlist)
df <-select(df, -index, -authorlist, -authoridlist, -access.type)
glimpse(df)
df %>% quantile(tc, c(0.75))
quantile(df$tc, c(0.75))
filter(df, tc.isna())
filter(df, is.na(tc))
filter(df, ~is.na(tc))
filter(df, !is.na(tc))
df<-filter(df, !is.na(tc))
quantile(df$tc, c(0.75))
ifelse(df$tc>=20, 1, 0)
mean(ifelse(df$tc>=20, 1, 0))
mean(ifelse(df$tc>20, 1, 0))
df$tcB<-ifelse(df$tc>=20, 1, 0)
glimpse(df)
df_train<-filter(df, split==0)
df_dev<-filter(df,split==1)
library(glmnet)
set.seed(1)
model<-model.matrix( ~ ., data=select(df_train, -tc, -tcB))
fit=cv.glmnet(x=model, y=df$tcB, alpha=1, family='binomial')
select(df_train, -tc, -tcB)
set.seed(1)
model<-model.matrix( ~ ., data=select(df_train, -tc, -tcB))
fit=cv.glmnet(x=model, y=df_train$tcB, alpha=1, family='binomial')
set.seed(1)
model<-model.matrix( tcB~ . -tc , data=df_train)
fit=cv.glmnet(model, alpha=1, family='binomial')
set.seed(1)
model<-model.matrix( tcB~ . -tc , data=df_train)
fit=cv.glmnetx=(model, y=df_train$tcB alpha=1, family='binomial')
set.seed(1)
model<-model.matrix( tcB~ . -tc , data=df_train)
fit=cv.glmnetx=(model, y=df_train$tcB, alpha=1, family='binomial')
set.seed(1)
model<-model.matrix( tcB~ . -tc , data=df_train)
fit=cv.glmnetx=(x=model, y=df_train$tcB, alpha=1, family='binomial')
model.matrix(~ ., data=select(df_train, -tc, -tcB))
model<-model.matrix(~ ., data=select(df_train, -tc, -tcB))
set.seed(1)
fit=cv.glmnet(x=model, y=df_train$tcB, alpha=1, family='binomial')
model<-model.matrix(~ ., data=select(df_train, -tc, -tcB, -pagecount))
set.seed(1)
fit=cv.glmnet(x=model, y=df_train$tcB, alpha=1, family='binomial')
colnames(df)[colSums(is.na(df)) > 0]
filter(df, is.na(ik_humans))
filter(df, is.na(pagecount))
filter(df, is.na(ak_art))
glimpse(df)
glimpse(df)
filter(df, is.na(ak_medic))
filter(df, is.na(ak_health))
filter(df, is.na(ak_af))
filter(df, is.na(ik_cation))
replace_na(df$pagecount, 1)
df$pagecount=replace_na(df$pagecount, 1)
df<-replace_na(df, 0)
df<-replace_na(df, rep(0,545))
df<-replace_na(df, list(rep(0,545)))
df_train<-filter(df, split==0)
df_dev<-filter(df,split==1)
model<-model.matrix(~ ., data=select(df_train, -tc, -tcB, -pagecount))
set.seed(1)
fit=cv.glmnet(x=model, y=df_train$tcB, alpha=1, family='binomial')
colnames(df)[colSums(is.na(df)) > 0]
df[is.na(df)] = 0
colnames(df)[colSums(is.na(df)) > 0]
model<-model.matrix(~ ., data=select(df_train, -tc, -tcB, -pagecount))
set.seed(1)
fit=cv.glmnet(x=model, y=df_train$tcB, alpha=1, family='binomial')
df_train<-filter(df, split==0)
df_dev<-filter(df,split==1)
model<-model.matrix(~ ., data=select(df_train, -tc, -tcB, -pagecount))
set.seed(1)
fit=cv.glmnet(x=model, y=df_train$tcB, alpha=1, family='binomial')
plot(fit)
knitr::opts_chunk$set(echo = TRUE)
df <- read.csv('dfscopus-2.csv')
df <-select(df, -index, -authorlist, -authoridlist, -access.type)
df<-filter(df, !is.na(tc))
quantile(df$tc, c(0.75))
library(tidyverse)
df <-select(df, -index, -authorlist, -authoridlist, -access.type)
df<-filter(df, !is.na(tc))
quantile(df$tc, c(0.75))
df$tcB<-ifelse(df$tc>=20, 1, 0)
df[is.na(df)] = 0
df <- read.csv('dfscopus-2.csv')
df <-select(df, -index, -authorlist, -authoridlist, -access.type)
df<-filter(df, !is.na(tc))
df$tcB<-ifelse(df$tc>=20, 1, 0)
df$pagecount=replace_na(df$pagecount, 1)
df[is.na(df)] = 0
df_train<-filter(df, split==0)
df_dev<-filter(df,split==1)
library(glmnet)
set.seed(1)
model<-model.matrix(~ ., data=select(df_train, -tc, -tcB, -pagecount))
fit=cv.glmnet(x=model, y=df_train$tcB, alpha=1, family='binomial')
plot(fit)
bestlam=fit$lambda.min
bestlam
i <- which(fit$lambda == fit$lambda.min)
mse.min <- fit$cvm[i]
mse.min
coef(glmnet(model, df_train$tcB, alpha=1, lambda=bestlam))
bestlam=fit$lambda.min
bestlam
coef(glmnet(model, df_train$tcB, alpha=1, lambda=bestlam))
set.seed(1)
model<-model.matrix(~ ., data=select(df_train, -tc, -tcB, -pagecount, -split)) #don't forget to remove variables you don't want to regress on
fit=cv.glmnet(x=model, y=df_train$tcB, alpha=1, family='binomial')
plot(fit)
bestlam=fit$lambda.min
bestlam
coef(glmnet(model, df_train$tcB, alpha=1, lambda=bestlam))
testpred=predict(fit, newx = model.matrix(tcB ~ . -pagecount -tc -split, data=df_dev), s=bestlam)
testpred=predict(fit, newx = model.matrix(tcB ~ . -pagecount -tc -split, data=df_dev, type="response"), s=bestlam)
